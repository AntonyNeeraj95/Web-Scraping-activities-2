{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets first connect to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.naukri.com')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching the jobs for data analyst role in bangalore\n",
    "search_job=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "search_btn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "job_titles=[]\n",
    "company_name=[]\n",
    "locations_list=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all tags having job titles\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "#now extxracting the text from these tags\n",
    "for i in titles:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "#extracting all tags having company name\n",
    "companies=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "#extracting text from these tags\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "\n",
    "#extracting all tags having location details\n",
    "locations=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "#extracting the text from these tags\n",
    "for i in locations:\n",
    "    locations_list.append(i.text)\n",
    "    \n",
    "#extracting all tags having experience required list\n",
    "experience=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experience:\n",
    "    experience_list.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>experience_required</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst - SAP</td>\n",
       "      <td>Boston Scientific Corporation</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Pune, Delhi, Bengaluru, Gurgaon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hiring Data Analysts on Contract</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Schneider Electric</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Shell India Markets Private Limited</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst -Azure Data lake, Azure Data factory</td>\n",
       "      <td>Mindtree Limited</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Business / Data Analyst</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst - O2C - Bangalore</td>\n",
       "      <td>RANDSTAD INDIA PVT LTD</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Intern Data Analyst</td>\n",
       "      <td>Outsource Big Data</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NiFi Data Analyst</td>\n",
       "      <td>Capgemini Technology Services India Limited</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>GlaxoSmithKline Pharmaceuticals Limited</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "      <td>Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                 Data Analyst - SAP   \n",
       "1                   Hiring Data Analysts on Contract   \n",
       "2                                Senior Data Analyst   \n",
       "3                                       Data Analyst   \n",
       "4  Data Analyst -Azure Data lake, Azure Data factory   \n",
       "5                            Business / Data Analyst   \n",
       "6                     Data Analyst - O2C - Bangalore   \n",
       "7                                Intern Data Analyst   \n",
       "8                                  NiFi Data Analyst   \n",
       "9                                       Data Analyst   \n",
       "\n",
       "                                       company experience_required  \\\n",
       "0                Boston Scientific Corporation             3-5 Yrs   \n",
       "1            Flipkart Internet Private Limited             2-5 Yrs   \n",
       "2                           Schneider Electric             2-5 Yrs   \n",
       "3          Shell India Markets Private Limited             5-8 Yrs   \n",
       "4                             Mindtree Limited             5-9 Yrs   \n",
       "5                       IBM India Pvt. Limited             2-4 Yrs   \n",
       "6                       RANDSTAD INDIA PVT LTD             2-4 Yrs   \n",
       "7                           Outsource Big Data             0-1 Yrs   \n",
       "8  Capgemini Technology Services India Limited             4-6 Yrs   \n",
       "9      GlaxoSmithKline Pharmaceuticals Limited             4-6 Yrs   \n",
       "\n",
       "                              location  \n",
       "0      Pune, Delhi, Bengaluru, Gurgaon  \n",
       "1                            Bengaluru  \n",
       "2                Bengaluru / Bangalore  \n",
       "3                            Bengaluru  \n",
       "4  Chennai, Pune, Bengaluru, Hyderabad  \n",
       "5                            Bengaluru  \n",
       "6                            Bengaluru  \n",
       "7                            Bengaluru  \n",
       "8                            Bengaluru  \n",
       "9                            Bengaluru  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[0:10]\n",
    "jobs['company']=company_name[0:10]\n",
    "jobs['experience_required']=experience_list[0:10]\n",
    "jobs['location']=locations_list[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.naukri.com')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching the jobs for data scientist role in bangalore\n",
    "search_job=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='qsb-location-sugg']\")\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "search_btn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "job_titles=[]\n",
    "company_name=[]\n",
    "locations_list=[]\n",
    "job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all tags having job titles\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "#now extxracting the text from these tags\n",
    "for i in titles:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "#extracting all tags having company name\n",
    "companies=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "#extracting text from these tags\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "\n",
    "#extracting all tags having location details\n",
    "locations=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "#extracting the text from these tags\n",
    "for i in locations:\n",
    "    locations_list.append(i.text)\n",
    "    \n",
    "#extracting all tags having jobs description\n",
    "buttons=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for button in buttons[:10]:\n",
    "    button.click()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1=('https://www.naukri.com/job-listings-data-scientist-python-matlab-machine-learning-algorithms-wrackle-technologies-pvt-ltd-bengaluru-bangalore-3-to-8-years-080221905947?src=jobsearchDesk&sid=16132189285466039&xp=1&px=1')\n",
    "driver.get(url1)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "job_description.append(description.text)\n",
    "\n",
    "url2=('https://www.naukri.com/job-listings-lead-data-scientist-machine-learning-data-mining-wrackle-technologies-pvt-ltd-bengaluru-bangalore-6-to-11-years-080221900886?src=jobsearchDesk&sid=16132189285466039&xp=2&px=1')\n",
    "driver.get(url2)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "job_description.append(description.text)\n",
    "\n",
    "url3=('https://www.naukri.com/job-listings-data-scientist-machine-learning-commerce-bu-blue-yonder-india-private-limited-bengaluru-bangalore-4-to-6-years-010221901825?src=jobsearchDesk&sid=16132189285466039&xp=3&px=1')\n",
    "driver.get(url3)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='clearboth description']\")\n",
    "job_description.append(description.text)\n",
    "\n",
    "url4=('https://www.naukri.com/job-listings-principal-data-scientist-machine-deep-learning-nlp-tensorflow-fidius-advisory-bengaluru-bangalore-8-to-13-years-070720900498?src=jobsearchDesk&sid=16132189285466039&xp=4&px=1')\n",
    "driver.get(url4)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "job_description.append(description.text)\n",
    "\n",
    "url5=('https://www.naukri.com/job-listings-data-scientist-atos-syntel-private-limited-chennai-pune-mumbai-bengaluru-bangalore-12-to-18-years-220719001543?src=jobsearchDesk&sid=16132189285466039&xp=5&px=1')\n",
    "driver.get(url5)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "job_description.append(description.text)\n",
    "\n",
    "url6=('https://www.naukri.com/job-listings-opening-for-sr-data-scientist-tech-mahindra-tech-mahindra-ltd-pune-bengaluru-bangalore-12-to-20-years-100221001125?src=jobsearchDesk&sid=16132189285466039&xp=6&px=1')\n",
    "driver.get(url6)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='f14 lh18 alignJ disc-li']/ul\")\n",
    "job_description.append(description.text)\n",
    "\n",
    "url7=('https://www.naukri.com/job-listings-opening-for-sr-data-scientist-tech-mahindra-tech-mahindra-ltd-pune-bengaluru-bangalore-12-to-20-years-090221006526?src=jobsearchDesk&sid=16132189285466039&xp=7&px=1')\n",
    "driver.get(url7)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='f14 lh18 alignJ disc-li']/ul\")\n",
    "job_description.append(description.text)\n",
    "\n",
    "url8=('https://www.naukri.com/job-listings-senior-data-scientist-ces-it-ltd-cmmi-level-5-ces-ltd-chennai-pune-delhi-ncr-mumbai-bengaluru-bangalore-hyderabad-secunderabad-kolkata-2-to-7-years-151220006902?src=jobsearchDesk&sid=16132816355289078&xp=8&px=1')\n",
    "driver.get(url8)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "job_description.append(description.text)\n",
    "\n",
    "url9=('https://www.naukri.com/job-listings-data-scientist-and-senior-data-scientist-academic-operations-randstad-india-pvt-ltd-bengaluru-bangalore-2-to-5-years-080221007079?src=jobsearchDesk&sid=16132816355289078&xp=9&px=1')\n",
    "driver.get(url9)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "job_description.append(description.text)\n",
    "\n",
    "url10=('https://www.naukri.com/job-listings-sr-analyst-data-scientist-mindtree-limited-bengaluru-bangalore-10-to-15-years-040221502130?src=jobsearchDesk&sid=16132816355289078&xp=10&px=1')\n",
    "driver.get(url10)\n",
    "description=driver.find_element_by_xpath(\"//div[@class='dang-inner-html']\")\n",
    "job_description.append(description.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist - Data Mining/ Machine Learning/ Statistical Analysis\\n\\nRequirements :\\n\\n- 3-9 years of strong experience in data mining, machine learning, and statistical analysis.\\n\\n- BS/MS/Ph.D. in Computer Science, Statistics, Applied Math, or related areas from Premier institutes ( only IITs / IISc / BITS / Top NITs or top US university should apply)\\n\\n- Ability to lead and deliver in a fast-paced start-up environment.\\n\\n- Fluency in tools such as Matlab, Python, etc.\\n\\n- Strong intuition for data and Keen aptitude on large scale data analysis\\n\\n- Excellent written and verbal communication skills.\\n\\n- Ability to collaborate across teams and strong interpersonal skills.',\n",
       " 'Roles and Responsibilities\\nRequirements :\\n\\n- 6-9 years of strong experience in data mining, machine learning and statistical analysis.\\n\\n- BS/ MS/ PhD in Computer Science, Statistics, Applied Math, or related areas from Premier institutes ( only IITs / IISc / BITS / Top NITs or top US university should apply)\\n\\n- Ability to lead and deliver in a fast-paced start-up environment.\\n\\n- Fluency in tools such as Python/ R/ Matlab etc.\\n\\n- Strong intuition for data and Keen aptitude on large scale data analysis\\n\\n- Excellent written and verbal communication skills.\\n\\n- Ability to collaborate across teams and strong interpersonal skills.',\n",
       " 'Roles and Responsibilities\\nUnder guidance, or independently, design and implement machine learning models by\\nProcuring data from platform, client and public data sources\\nImplementing data enrichment and cleansing routines\\nImplementing features, preparing modeling data sets, feature selection, etc.\\nEvaluating candidate models, selecting and reporting on test performance of final one\\nEnsuring proper runtime deployment of models, and\\nImplementing runtime monitoring of model inputs and performance in order to ensure continued model stability\\nWork with product, sales and engineering teams helping shape up the final solution\\nUse data to understand patterns, come up with and test hypothesis; iterate\\nHelp prepare sales materials, estimate hardware requirements, etc.',\n",
       " \"Job Description :\\n- We are looking for a researcher who specializes in building personalization/recommender systems algorithm (ML APIs) for the applications mentioned above and work with our engineers to deploy them as scale.\\nWhat you will do :\\n- Apply your research expertise to build our ML-driven recommender system products, help us develop new solutions and unlock new directions, as well as analyse and optimise the systems we already. \\n- You'll work closely with product teams and mentor them on best practices for modern ML, and keep the wider team informed on the state-of-the-art. In addition, you will be in a strategic position to influence future roadmaps for recommender system products.\\n- Collaborate with a cross-functional agile team spanning user research, design, data science, product management, and engineering to build new product features that advance our mission to connect artists and fans in personalized and relevant ways.\\n- Prototype new approaches and production-ize solutions at scale for our hundreds of thousands of active users. Help drive optimization, testing, and tools to improve quality.\\nRequirements :\\n- Master, Post-graduate or Ph.D. in computer science, machine learning, information retrieval, recommendation systems, natural language processing, statistics, math, engineering, operations research, or another quantitative discipline; or equivalent work experience.\\n- Good theoretical grounding in core machine learning concepts and techniques.\\n- Ability to perform comprehensive literature reviews and provide critical feedback on state-of-the-art solutions and how they may fit different operating constraints.\\n- Experience with a number of ML techniques and frameworks, e.g. Natural Language Processing, Recommender Systems, sampling, linear regression, decision trees, SVMs, deep neural networks, etc.\\n- Familiarity with one or more Deep learning software frameworks such as Tensorflow, PyTorch.\",\n",
       " 'Working experience in Artificial Intelligence, Python, R, Machine Learning\\nExperience in data mining, Strong math skills (e.g. statistics, algebra)\\nStrong programming skills in: R, Python and familiarity with Java, Scala, C - DB/NoSql - MongoDB, Neo4J, MySql. Cassandra, DynamoDB, Redshift\\nExperience on Hadoop Map reduce, Pig, Hive, Mahout and Apache Spark, H20\\nStrong experience in Data warehousing, ETL, BI (e.g. Tableau, Power BI) and Data Visualization tools (matplotlib, D3.js, Plotly.js, Shiny etc)\\nExperience with Deep Learning tools Tensorflow, Theano, Caffe etc. - Elastic Search, NLP background and Machine Learning Platforms\\nExperienced in deployment of High performance, Scalable Big Data Hadoop clusters and Web applications on cloud infrastructure (Azure, AWS, Bluemix etc)\\nExperience in neural networks, regression, classification and clustering\\nDeep industry knowledge on any of the following: Banking, Insurance, Retail Manufacturing\\nDeep understanding of Statistical algorithms: Linear and Non-Linear models, classification problem, optimization techniques, Market mix models, A/B Testing and campaign management, Feature ranking/selection techniques, supervised/unsupervised learning, Collaborative filtering, Apriori Market Basket analysis, SVM, Gradient boosting, Survival analysis etc.\\nTo help designing, innovating and building our next generation ML architecture\\nFull time programming experience within an operation or technical department.\\nIdentify valuable data sources and automate collection processes\\nUndertake pre-processing of structured and unstructured data\\nAnalyze large amounts of information to discover trends and patterns\\nBuild predictive models and machine-learning algorithms\\nCombine models through ensemble modelling\\nPresent information using data visualization techniques\\nPropose solutions and strategies to business challenges\\nCollaborate with engineering and product development teams\\nMentor others in the use of AI/Machine Learning',\n",
       " 'BE or MS or PhD degree in Computer Science, Artificial Intelligence, Machine Learning, or a related technical field.\\nDeep learning experience with Tensorflow and Keras or Pytorch is must\\nExperience with the following is preferred\\nDifferent data processing libraries such as Numpy, Scipy, Pandas, Matplotlib, Scikit-learn\\nExperience with one or more general purpose programming languages including but not limited to: Python (Must), R, Java, or C/C++.\\nDifferent Networks CNN, RNN, LSTM, Transformer Based models, GAN, Reinforcement Learning\\nFamiliarity with different model architectures AlexNetm, ResNet, MobileNet etc., in Computer Vision & BiLSTM, Bert family models.\\nExperience with distribution strategies for data and model parallelism\\nDevelopment of mobile applications with tensorflow (TF Lite)\\nExperience in the development of End-to-end production based models. Cloud deployment of any machine learning models\\nUnderstanding of advanced machine learning data pipelines components including Data Versioning, Code Versioning, Solution Versioning, Drift detection, Model Interpretability & Explainability.\\nExperience with Structured Data Machine Learning techniques\\nUsed AutoML techniques.\\nWorked or knowledge on distributed computing example Spark.\\nExperience with open source systems such as MLFlow, DVC, PyCaret, Prophet etc.',\n",
       " 'BE or MS or PhD degree in Computer Science, Artificial Intelligence, Machine Learning, or a related technical field.\\nDeep learning experience with Tensorflow and Keras or Pytorch is must\\nExperience with the following is preferred\\nDifferent data processing libraries such as Numpy, Scipy, Pandas, Matplotlib, Scikit-learn\\nExperience with one or more general purpose programming languages including but not limited to: Python (Must), R, Java, or C/C++.\\nDifferent Networks CNN, RNN, LSTM, Transformer Based models, GAN, Reinforcement Learning\\nFamiliarity with different model architectures AlexNetm, ResNet, MobileNet etc., in Computer Vision & BiLSTM, Bert family models.\\nExperience with distribution strategies for data and model parallelism\\nDevelopment of mobile applications with tensorflow (TF Lite)\\nExperience in the development of End-to-end production based models. Cloud deployment of any machine learning models\\nUnderstanding of advanced machine learning data pipelines components including Data Versioning, Code Versioning, Solution Versioning, Drift detection, Model Interpretability & Explainability.\\nExperience with Structured Data Machine Learning techniques\\nUsed AutoML techniques.\\nWorked or knowledge on distributed computing example Spark.\\nExperience with open source systems such as MLFlow, DVC, PyCaret, Prophet etc.',\n",
       " 'Roles and Responsibilities\\n\\nMust have strong Python Programming Skills\\nStrong analytical & algorithm development skills\\nLogical and Analytical skills must be really strong\\nMust have worked in DeepLearning Efforts - Especially computer vision.\\nMust have experinece with Object Detection - Custom model training for Object detection\\nShould have experience with atleast one or more of these - Tensorflow, Keras, PyTorch\\n\\nPrimary Skills - Python + tensorflow - Keras / PyTorch, OpenCV\\nPerks and Benefits\\n\\nKindly share your resume to kandavelkumar.lakshmanan @cesltd.com',\n",
       " \"We are hiring Data Scientist and Senior Data Scientist Academic Operations for our leading EdTech Client.\\n\\n\\n\\nFill the google form - https://tinyurl.com/JobAppForm4\\n\\nJob Responsibilities:\\nYour primary job responsibility will include (and not limited to):\\nOwn the student's learning outcomes by providing them with support on the topics covered in the\\ncurriculum.\\nInvolve in the residency class room sessions to facilitate lectures and lab sessions\\nBe the first point contact for participants for academic queries and manage discussion\\ngroups\\nMonitor participants academic performance and make learning interventions in the\\nform of remedial sessions, coaching and mentoring\\n\\nCoordinate with faculty to create best in class learning material - video, reading material,\\nassignments, exams\\nDesign and conduct examinations to measure the learning outcome of participants\\nIdentify & Solve interesting problems involving rich datasets in various domains.\\nAccordingly, create capstone projects on the evolving use cases in the industry\\nIdentify key emerging trends in the industry and maintain a rich reference material\\nAssist program director and senior operations and academics managers in planning\\non-campus sessions, preparing schedules, evaluation and grading\\nIdentify key reporting metric sand create dashboards to enable quick decision making\\nAutomation of manual data collection, data cleansing and exploratory data analysis\\nCreate and maintain business and technical requirements\\nIdentify technical solutions and perform feasibility analysis\\nCreate technical roadmaps for the operations Team\\nManage, identify and suggest processes for smoother program management to\\nensure a consistent and trouble free learning experience\\nCoordinate with IT and Admin to ensure smooth execution at various locations\\nTravel to other cities if required to manage residencies\\n\\nRelevant Background:\\nGraduate with an exceptional academic track record\\nCompetency: (Top 3)\\n1. Passion for learning and having great learning outcomes\\n2. Ability to multitask and coordinate with multiple stakeholders\\n3. Excellent knowledge in python, tableau and ML concepts\",\n",
       " 'Role Description:\\nA Sr. Data Scientist who lead the development of analytics / machine learning / AI models for generating future prediction using data.\\nResponsibilities : - Understanding the project requirement\\n- Understand if there is any already existing ML model\\n- Leading analytics project & provide necessary guidance to the team\\n- Understand the data & doing EDA\\n- Feature engineering - Building Data Science models\\n- Validate & Piloting ML models\\n- Model tuning & improvement\\nRequired Skills:\\n- Sound theoretical knowledge in ML algorithm and their application\\n- Strong fundamental on statistics\\n- Experience in leading multiple data science projects\\n- Hands on experience on Machine learning / data science\\n- Strong stakeholder management skills\\n- Expert in Python - Hand on experience in SPARK Scala or PySpark\\nAdded Advantage:\\n- Hands on experience in Azure data bricks\\n- Working experience in CPG domain\\nSkills Required:\\nDATA ANALYSIS, Machine learning, Python, Databricks']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full description of all jobs displayed as a single list\n",
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>job_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist/Data Analyst-immediate</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>Data Scientist - Data Mining/ Machine Learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist - Python/ MATLAB/ Machine Learn...</td>\n",
       "      <td>Wrackle Technologies Pvt Ltd</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Roles and Responsibilities\\nRequirements :\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Data Scientist - Machine Learning/ Data M...</td>\n",
       "      <td>Wrackle Technologies Pvt Ltd</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Roles and Responsibilities\\nUnder guidance, or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - Machine Learning (Commerce BU)</td>\n",
       "      <td>BLUE YONDER INDIA PRIVATE LIMITED</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Job Description :\\n- We are looking for a rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Atos Syntel Private Limited</td>\n",
       "      <td>Chennai, Pune, Mumbai, Bengaluru</td>\n",
       "      <td>Working experience in Artificial Intelligence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Opening For Sr. Data Scientist @ Tech Mahindra</td>\n",
       "      <td>Tech Mahindra Ltd.</td>\n",
       "      <td>Pune, Bengaluru</td>\n",
       "      <td>BE or MS or PhD degree in Computer Science, Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Opening For Sr. Data Scientist @ Tech Mahindra</td>\n",
       "      <td>Tech Mahindra Ltd.</td>\n",
       "      <td>Pune, Bengaluru</td>\n",
       "      <td>BE or MS or PhD degree in Computer Science, Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist - NLP/ Python/ R</td>\n",
       "      <td>AVI Consulting LLP</td>\n",
       "      <td>Bengaluru, Hyderabad</td>\n",
       "      <td>Roles and Responsibilities\\n\\nMust have strong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist | CES IT LTD | CMMI Level 5</td>\n",
       "      <td>CES Ltd.</td>\n",
       "      <td>Chennai, Pune, Delhi NCR, Mumbai, Bengaluru, H...</td>\n",
       "      <td>We are hiring Data Scientist and Senior Data S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sr Data Scientist</td>\n",
       "      <td>Concentrix Daksh Services India Private Limited.</td>\n",
       "      <td>Pune, Mumbai, Bengaluru, Gurgaon, Kolkata</td>\n",
       "      <td>Role Description:\\nA Sr. Data Scientist who le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0              Data Scientist/Data Analyst-immediate   \n",
       "1  Data Scientist - Python/ MATLAB/ Machine Learn...   \n",
       "2  Lead Data Scientist - Machine Learning/ Data M...   \n",
       "3    Data Scientist - Machine Learning (Commerce BU)   \n",
       "4                                     Data Scientist   \n",
       "5     Opening For Sr. Data Scientist @ Tech Mahindra   \n",
       "6     Opening For Sr. Data Scientist @ Tech Mahindra   \n",
       "7             Senior Data Scientist - NLP/ Python/ R   \n",
       "8  Senior Data Scientist | CES IT LTD | CMMI Level 5   \n",
       "9                                  Sr Data Scientist   \n",
       "\n",
       "                                            company  \\\n",
       "0                Inflexion Analytix Private Limited   \n",
       "1                      Wrackle Technologies Pvt Ltd   \n",
       "2                      Wrackle Technologies Pvt Ltd   \n",
       "3                 BLUE YONDER INDIA PRIVATE LIMITED   \n",
       "4                       Atos Syntel Private Limited   \n",
       "5                                Tech Mahindra Ltd.   \n",
       "6                                Tech Mahindra Ltd.   \n",
       "7                                AVI Consulting LLP   \n",
       "8                                          CES Ltd.   \n",
       "9  Concentrix Daksh Services India Private Limited.   \n",
       "\n",
       "                                            location  \\\n",
       "0                Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1                                          Bengaluru   \n",
       "2                                          Bengaluru   \n",
       "3                                          Bengaluru   \n",
       "4                   Chennai, Pune, Mumbai, Bengaluru   \n",
       "5                                    Pune, Bengaluru   \n",
       "6                                    Pune, Bengaluru   \n",
       "7                               Bengaluru, Hyderabad   \n",
       "8  Chennai, Pune, Delhi NCR, Mumbai, Bengaluru, H...   \n",
       "9          Pune, Mumbai, Bengaluru, Gurgaon, Kolkata   \n",
       "\n",
       "                                     job_description  \n",
       "0  Data Scientist - Data Mining/ Machine Learning...  \n",
       "1  Roles and Responsibilities\\nRequirements :\\n\\n...  \n",
       "2  Roles and Responsibilities\\nUnder guidance, or...  \n",
       "3  Job Description :\\n- We are looking for a rese...  \n",
       "4  Working experience in Artificial Intelligence,...  \n",
       "5  BE or MS or PhD degree in Computer Science, Ar...  \n",
       "6  BE or MS or PhD degree in Computer Science, Ar...  \n",
       "7  Roles and Responsibilities\\n\\nMust have strong...  \n",
       "8  We are hiring Data Scientist and Senior Data S...  \n",
       "9  Role Description:\\nA Sr. Data Scientist who le...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[0:10]\n",
    "jobs['company']=company_name[0:10]\n",
    "jobs['location']=locations_list[0:10]\n",
    "jobs['job_description']=job_description[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: In this question you have to scrape data using the filters available on the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.naukri.com')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching the jobs for data scientist role in bangalore\n",
    "search_job=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "search_btn=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying filter for city\n",
    "btn=driver.find_element_by_xpath(\"//span[@title='Delhi/NCR']\")\n",
    "btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying filter for salary\n",
    "btn=driver.find_element_by_xpath(\"//span[@title='3-6 Lakhs']\")\n",
    "btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "job_titles=[]\n",
    "company_name=[]\n",
    "locations_list=[]\n",
    "experience_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all tags having job titles\n",
    "titles=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "#now extxracting the text from these tags\n",
    "for i in titles:\n",
    "    job_titles.append(i.text)\n",
    "\n",
    "#extracting all tags having company name\n",
    "companies=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "#extracting text from these tags\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "\n",
    "#extracting all tags having location details\n",
    "locations=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "#extracting the text from these tags\n",
    "for i in locations:\n",
    "    locations_list.append(i.text)\n",
    "    \n",
    "#extracting all tags having experience required list\n",
    "experience=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']/span[1]\")\n",
    "for i in experience:\n",
    "    experience_list.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>experience_required</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Amity University</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "      <td>Faridabad, Delhi NCR, Ghaziabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Trainer - online</td>\n",
       "      <td>IIBM Institute of Business Management</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "      <td>Chennai, Pune, Delhi NCR, Mumbai, Bengaluru, H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Extramarks Education India Pvt Ltd</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "      <td>Delhi NCR, Ghaziabad, Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Extramarks Education India Pvt Ltd</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "      <td>Delhi NCR, Ghaziabad, Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Unacademy</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "      <td>Bengaluru, Noida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MIS Executive/Business Analyst</td>\n",
       "      <td>ApplyBoard</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>Gurgaon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>Chegg India Pvt Ltd</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Edunuts</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Splash M</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "      <td>Gurgaon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Snaphunt</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Delhi NCR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title                                company  \\\n",
       "0                  Data Scientist                       Amity University   \n",
       "1   Data Science Trainer - online  IIBM Institute of Business Management   \n",
       "2                    Data Analyst     Extramarks Education India Pvt Ltd   \n",
       "3                    Data Analyst     Extramarks Education India Pvt Ltd   \n",
       "4                   Data Engineer                              Unacademy   \n",
       "5  MIS Executive/Business Analyst                             ApplyBoard   \n",
       "6            Senior Data Engineer                    Chegg India Pvt Ltd   \n",
       "7                    Data Analyst                                Edunuts   \n",
       "8                   Data Engineer                               Splash M   \n",
       "9                   Data Engineer                               Snaphunt   \n",
       "\n",
       "  experience_required                                           location  \n",
       "0             6-8 Yrs                    Faridabad, Delhi NCR, Ghaziabad  \n",
       "1             1-6 Yrs  Chennai, Pune, Delhi NCR, Mumbai, Bengaluru, H...  \n",
       "2             2-3 Yrs                        Delhi NCR, Ghaziabad, Noida  \n",
       "3             2-3 Yrs                        Delhi NCR, Ghaziabad, Noida  \n",
       "4             1-4 Yrs                                   Bengaluru, Noida  \n",
       "5             0-3 Yrs                                            Gurgaon  \n",
       "6             2-7 Yrs                                              Delhi  \n",
       "7             2-5 Yrs                                              Delhi  \n",
       "8             2-4 Yrs                                            Gurgaon  \n",
       "9             2-5 Yrs                                          Delhi NCR  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[0:10]\n",
    "jobs['company']=company_name[0:10]\n",
    "jobs['experience_required']=experience_list[0:10]\n",
    "jobs['location']=locations_list[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.glassdoor.co.in/index.htm')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching the jobs for data analyst role in bangalore\n",
    "search_job=driver.find_element_by_name(\"sc.keyword\")\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='sc.location']\")\n",
    "search_loc.send_keys('Noida')\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto SearchStyles__newSearchButton css-iixdfr']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 3 empty lists\n",
    "company_name=[]\n",
    "days_posted=[]\n",
    "rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all tags having company name\n",
    "companies=driver.find_elements_by_xpath(\"//div[@class='d-flex justify-content-between align-items-start']\")\n",
    "#extracting text from these tags\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "\n",
    "#extracting all tags having the details of no of days before posted \n",
    "days=driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob']\")\n",
    "#extracting text from these tags\n",
    "for i in days:\n",
    "    days_posted.append(i.text)\n",
    "\n",
    "#extracting all tags having rating of the company\n",
    "ratng=driver.find_elements_by_xpath(\"//span[@class='css-19pjha7 e1cjmv6j1']\")\n",
    "#extracting text from these tags\n",
    "for i in ratng:\n",
    "    rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>days_posted</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>1d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BlackRock</td>\n",
       "      <td>1d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abc consultants</td>\n",
       "      <td>6d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adobe</td>\n",
       "      <td>8d</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biz2Credit Inc</td>\n",
       "      <td>14d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brickred</td>\n",
       "      <td>25d</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Priority Vendor</td>\n",
       "      <td>6d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gauge Data Solutions</td>\n",
       "      <td>6d</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Healtheoz India</td>\n",
       "      <td>6d</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Terra Economics &amp; Analytics Lab (TEAL)</td>\n",
       "      <td>7d</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  company days_posted rating\n",
       "0                               BlackRock          1d    4.1\n",
       "1                               BlackRock          1d    4.1\n",
       "2                         abc consultants          6d    4.1\n",
       "3                                   Adobe          8d    4.4\n",
       "4                          Biz2Credit Inc         14d    3.7\n",
       "5                                Brickred         25d    3.9\n",
       "6                         Priority Vendor          6d    3.7\n",
       "7                    Gauge Data Solutions          6d    3.1\n",
       "8                         Healtheoz India          6d    4.8\n",
       "9  Terra Economics & Analytics Lab (TEAL)          7d    4.9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['company']=company_name[0:10]\n",
    "jobs['days_posted']=days_posted[0:10]\n",
    "jobs['rating']=rating[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.glassdoor.co.in/Salaries/index.htm')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching the jobs for data analyst role in bangalore\n",
    "search_job=driver.find_element_by_name(\"sc.keyword\")\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "search_loc=driver.find_element_by_xpath(\"//input[@id='LocationSearch']\")\n",
    "search_loc.send_keys('Noida')\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "company_name=[]\n",
    "min_salary=[]\n",
    "max_salary=[]\n",
    "avg_salary=[]\n",
    "rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting all tags having company name\n",
    "companies=driver.find_elements_by_xpath(\"//p[@class='m-0 ']\")\n",
    "#extracting text from these tags\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "\n",
    "#extracting all tags having minimum salary\n",
    "min_sal=driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__rangeBar undefined undefined ']/div[2]/span[1]\")\n",
    "#extracting text from these tags\n",
    "for i in min_sal:\n",
    "    min_salary.append(i.text)\n",
    "\n",
    "#extracting all tags having maximum salary\n",
    "max_sal=driver.find_elements_by_xpath(\"//div[@class='common__RangeBarStyle__rangeBar undefined undefined ']/div[2]/span[2]\")\n",
    "#extracting text from these tags\n",
    "for i in max_sal:\n",
    "    max_salary.append(i.text)\n",
    "\n",
    "#extracting all tags having average salary\n",
    "avg_sal=driver.find_elements_by_xpath(\"//div[@class='col-2 d-none d-md-flex flex-row justify-content-end']/strong[1]\")\n",
    "#extracting text from these tags\n",
    "for i in avg_sal:\n",
    "    avg_salary.append(i.text)\n",
    "    \n",
    "\n",
    "#extracting all tags having rating of the company\n",
    "ratng=driver.find_elements_by_xpath(\"//p[@class='css-1uyte9r css-1kuy7z7 m-0 '][1]\")\n",
    "#extracting text from these tags\n",
    "for i in ratng:\n",
    "    rating.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>average_salary</th>\n",
       "      <th>minimium_salary</th>\n",
       "      <th>maximum_salary</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>₹ 12,64,182</td>\n",
       "      <td>₹450K</td>\n",
       "      <td>₹11,630K</td>\n",
       "      <td>13 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>₹ 7,30,968</td>\n",
       "      <td>₹350K</td>\n",
       "      <td>₹1,614K</td>\n",
       "      <td>12 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>₹ 6,00,000</td>\n",
       "      <td>₹336K</td>\n",
       "      <td>₹1,010K</td>\n",
       "      <td>11 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>₹ 9,94,055</td>\n",
       "      <td>₹577K</td>\n",
       "      <td>₹2,215K</td>\n",
       "      <td>10 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IBM</td>\n",
       "      <td>₹ 7,39,040</td>\n",
       "      <td>₹587K</td>\n",
       "      <td>₹2,732K</td>\n",
       "      <td>10 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>₹ 13,37,114</td>\n",
       "      <td>₹717K</td>\n",
       "      <td>₹1,575K</td>\n",
       "      <td>9 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>₹ 7,80,374</td>\n",
       "      <td>₹502K</td>\n",
       "      <td>₹1,152K</td>\n",
       "      <td>8 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Innovaccer</td>\n",
       "      <td>₹ 11,98,792</td>\n",
       "      <td>₹621K</td>\n",
       "      <td>₹1,696K</td>\n",
       "      <td>7 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cognizant Technology Solutions</td>\n",
       "      <td>₹ 10,08,143</td>\n",
       "      <td>₹793K</td>\n",
       "      <td>₹1,264K</td>\n",
       "      <td>6 salaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EXL Service</td>\n",
       "      <td>₹ 11,34,989</td>\n",
       "      <td>₹576K</td>\n",
       "      <td>₹1,500K</td>\n",
       "      <td>6 salaries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          company average_salary minimium_salary  \\\n",
       "0                       Delhivery    ₹ 12,64,182           ₹450K   \n",
       "1              Ericsson-Worldwide     ₹ 7,30,968           ₹350K   \n",
       "2       Tata Consultancy Services     ₹ 6,00,000           ₹336K   \n",
       "3                       Accenture     ₹ 9,94,055           ₹577K   \n",
       "4                             IBM     ₹ 7,39,040           ₹587K   \n",
       "5              UnitedHealth Group    ₹ 13,37,114           ₹717K   \n",
       "6              Valiance Solutions     ₹ 7,80,374           ₹502K   \n",
       "7                      Innovaccer    ₹ 11,98,792           ₹621K   \n",
       "8  Cognizant Technology Solutions    ₹ 10,08,143           ₹793K   \n",
       "9                     EXL Service    ₹ 11,34,989           ₹576K   \n",
       "\n",
       "  maximum_salary       rating  \n",
       "0       ₹11,630K  13 salaries  \n",
       "1        ₹1,614K  12 salaries  \n",
       "2        ₹1,010K  11 salaries  \n",
       "3        ₹2,215K  10 salaries  \n",
       "4        ₹2,732K  10 salaries  \n",
       "5        ₹1,575K   9 salaries  \n",
       "6        ₹1,152K   8 salaries  \n",
       "7        ₹1,696K   7 salaries  \n",
       "8        ₹1,264K   6 salaries  \n",
       "9        ₹1,500K   6 salaries  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['company']=company_name[0:10]\n",
    "jobs['average_salary']=avg_salary[0:10]\n",
    "jobs['minimium_salary']=min_salary[0:10]\n",
    "jobs['maximum_salary']=max_salary[0:10]\n",
    "jobs['rating']=rating[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.flipkart.com/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching the product in the webpage\n",
    "search_product=driver.find_element_by_name(\"q\")\n",
    "search_product.send_keys(\"sunglasses\")\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "Brand=[]\n",
    "Product_description=[]\n",
    "Price=[]\n",
    "Discount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting brand name of the products\n",
    "brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "#extracting text from these tags\n",
    "for i in brand:\n",
    "    Brand.append(i.text)\n",
    "    \n",
    "#extracting product description\n",
    "description=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")\n",
    "#extracting text from these tags\n",
    "for i in description:\n",
    "    Product_description.append(i.text)\n",
    "    \n",
    "#extracting price details of the product\n",
    "price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#exracting text from these tags\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "    \n",
    "#extracting discount details of the product\n",
    "discount=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "#extracting text from these tags\n",
    "for i in discount:\n",
    "    Discount.append(i.text)\n",
    "        \n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_page.click()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting brand name of the products\n",
    "brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "#extracting text from these tags\n",
    "for i in brand:\n",
    "    Brand.append(i.text)\n",
    "    \n",
    "#extracting product description\n",
    "description=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")\n",
    "#extracting text from these tags\n",
    "for i in description:\n",
    "    Product_description.append(i.text)\n",
    "    \n",
    "#extracting price details of the product\n",
    "price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#exracting text from these tags\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "    \n",
    "#extracting discount details of the product\n",
    "discount=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "#extracting text from these tags\n",
    "for i in discount:\n",
    "    Discount.append(i.text)\n",
    "        \n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting brand name of the products\n",
    "brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "#extracting text from these tags\n",
    "for i in brand[0:20]:\n",
    "    Brand.append(i.text)\n",
    "    \n",
    "#extracting product description\n",
    "description=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")\n",
    "#extracting text from these tags\n",
    "for i in description[0:20]:\n",
    "    Product_description.append(i.text)\n",
    "    \n",
    "#extracting price details of the product\n",
    "price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#exracting text from these tags\n",
    "for i in price[0:20]:\n",
    "    Price.append(i.text)\n",
    "    \n",
    "#extracting discount details of the product\n",
    "discount=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "#extracting text from these tags\n",
    "for i in discount[0:20]:\n",
    "    Discount.append(i.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>37% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>37% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹599</td>\n",
       "      <td>33% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection, Gradient Rectangular Sunglasses...</td>\n",
       "      <td>₹404</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>FDA COLLECTION</td>\n",
       "      <td>Gradient, UV Protection Round, Round Sunglasse...</td>\n",
       "      <td>₹161</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Night Vision, Riding Glasses Sp...</td>\n",
       "      <td>₹198</td>\n",
       "      <td>79% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Phenomenal</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Irayz</td>\n",
       "      <td>UV Protection Over-sized Sunglasses (Free Size)</td>\n",
       "      <td>₹279</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Criba</td>\n",
       "      <td>UV Protection Round, Aviator Sunglasses (58)</td>\n",
       "      <td>₹175</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brand                                Product_description Price  \\\n",
       "0   ROZZETTA CRAFT  UV Protection Retro Square Sunglasses (Free Size)  ₹499   \n",
       "1         Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...  ₹499   \n",
       "2         Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...  ₹499   \n",
       "3         Fastrack       UV Protection Aviator Sunglasses (Free Size)  ₹599   \n",
       "4   ROZZETTA CRAFT  UV Protection, Gradient Rectangular Sunglasses...  ₹404   \n",
       "..             ...                                                ...   ...   \n",
       "95  FDA COLLECTION  Gradient, UV Protection Round, Round Sunglasse...  ₹161   \n",
       "96           NuVew  UV Protection, Night Vision, Riding Glasses Sp...  ₹198   \n",
       "97      Phenomenal  UV Protection Retro Square Sunglasses (Free Size)  ₹399   \n",
       "98           Irayz    UV Protection Over-sized Sunglasses (Free Size)  ₹279   \n",
       "99           Criba       UV Protection Round, Aviator Sunglasses (58)  ₹175   \n",
       "\n",
       "   Discount  \n",
       "0   77% off  \n",
       "1   37% off  \n",
       "2   37% off  \n",
       "3   33% off  \n",
       "4   79% off  \n",
       "..      ...  \n",
       "95  83% off  \n",
       "96  79% off  \n",
       "97  80% off  \n",
       "98  78% off  \n",
       "99  64% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "sunglasses=pd.DataFrame({})\n",
    "sunglasses['Brand']=Brand\n",
    "sunglasses['Product_description']=Product_description\n",
    "sunglasses['Price']=Price\n",
    "sunglasses['Discount']=Discount\n",
    "sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.flipkart.com/apple-iphone-11-black-64-gb-includesearpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "Rating=[]\n",
    "Review_Summary=[]\n",
    "Full_Review=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#navigating to the ratings and review page\n",
    "btn=driver.find_element_by_xpath(\"//div[@class='_3UAT2v _16PBlm']/span\")\n",
    "btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "    \n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "    \n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "    \n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[11]\")\n",
    "next_page.click()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='row']/div[@class='_3LWZlK _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "\n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq' or @class='_3LWZlK _1rdVr6 _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "\n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "\n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "\n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "\n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "\n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "\n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "\n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the ratings of the product\n",
    "rating=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "#extracting text from these tags\n",
    "for i in rating:\n",
    "    Rating.append(i.text)\n",
    "\n",
    "#extracting review summary of the product\n",
    "rev_summ=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "#extracting text from these tags\n",
    "for i in rev_summ:\n",
    "    Review_Summary.append(i.text)\n",
    "\n",
    "#extracting full review of the  product\n",
    "full_rev=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div/div\")\n",
    "#exracting text from these tags\n",
    "for i in full_rev:\n",
    "    Full_Review.append(i.text)\n",
    "\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review_Summary</th>\n",
       "      <th>Full_Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Delightful</td>\n",
       "      <td>Nice camera for front and back.. The display i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>Best phone in this price segment people say th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>I got this beast today. And I must say the pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Got mobile @56,990 on 8 oct 2020 when all othe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Have used both iPhone X and iPhone XR and I ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating      Review_Summary  \\\n",
       "0       5    Perfect product!   \n",
       "1       5       Great product   \n",
       "2       5  Highly recommended   \n",
       "3       5    Perfect product!   \n",
       "4       5           Brilliant   \n",
       "..    ...                 ...   \n",
       "95      4          Delightful   \n",
       "96      5           Fabulous!   \n",
       "97      5            Terrific   \n",
       "98      5           Excellent   \n",
       "99      5            Terrific   \n",
       "\n",
       "                                          Full_Review  \n",
       "0   Amazing phone with great cameras and better ba...  \n",
       "1   Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "2   iphone 11 is a very good phone to buy only if ...  \n",
       "3   It’s a must buy who is looking for an upgrade ...  \n",
       "4   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "..                                                ...  \n",
       "95  Nice camera for front and back.. The display i...  \n",
       "96  Best phone in this price segment people say th...  \n",
       "97  I got this beast today. And I must say the pic...  \n",
       "98  Got mobile @56,990 on 8 oct 2020 when all othe...  \n",
       "99  Have used both iPhone X and iPhone XR and I ca...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "iphone11_review=pd.DataFrame({})\n",
    "iphone11_review['Rating']=Rating\n",
    "iphone11_review['Review_Summary']=Review_Summary\n",
    "iphone11_review['Full_Review']=Full_Review\n",
    "iphone11_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.flipkart.com/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching the product in the webpage\n",
    "search_product=driver.find_element_by_name(\"q\")\n",
    "search_product.send_keys(\"sneakers\")\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "Brand=[]\n",
    "Product_description=[]\n",
    "Price=[]\n",
    "Discount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting brand name of the products\n",
    "brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "#extracting text from these tags\n",
    "for i in brand:\n",
    "    Brand.append(i.text)\n",
    "    \n",
    "#extracting product description\n",
    "description=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")\n",
    "#extracting text from these tags\n",
    "for i in description:\n",
    "    Product_description.append(i.text)\n",
    "    \n",
    "#extracting price details of the product\n",
    "price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#exracting text from these tags\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "    \n",
    "#extracting discount details of the product\n",
    "discount=driver.find_elements_by_xpath(\"//div[@class='_25b18c']/div[3]\")\n",
    "#extracting text from these tags\n",
    "for i in discount:\n",
    "    if discount is not None:\n",
    "        Discount.append(i.text)\n",
    "    else:\n",
    "        Discount.append('0 %')\n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//a[@class='_1LKTO3']\")\n",
    "next_page.click()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting brand name of the products\n",
    "brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "#extracting text from these tags\n",
    "for i in brand:\n",
    "    Brand.append(i.text)\n",
    "    \n",
    "#extracting product description\n",
    "description=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")\n",
    "#extracting text from these tags\n",
    "for i in description:\n",
    "    Product_description.append(i.text)\n",
    "    \n",
    "#extracting price details of the product\n",
    "price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#exracting text from these tags\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "    \n",
    "#extracting discount details of the product\n",
    "discount=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "#extracting text from these tags\n",
    "for i in discount:\n",
    "    Discount.append(i.text)\n",
    "        \n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//nav[@class='yFHi8N']/a[12]\")\n",
    "next_page.click()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting brand name of the products\n",
    "brand=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "#extracting text from these tags\n",
    "for i in brand[0:20]:\n",
    "    Brand.append(i.text)\n",
    "    \n",
    "#extracting product description\n",
    "description=driver.find_elements_by_xpath(\"//div[@class='_2B099V']/a[1]\")\n",
    "#extracting text from these tags\n",
    "for i in description[0:20]:\n",
    "    Product_description.append(i.text)\n",
    "    \n",
    "#extracting price details of the product\n",
    "price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#exracting text from these tags\n",
    "for i in price[0:20]:\n",
    "    Price.append(i.text)\n",
    "    \n",
    "#extracting discount details of the product\n",
    "discount=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "#extracting text from these tags\n",
    "for i in discount[0:20]:\n",
    "    Discount.append(i.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>Combo Pack of 4 Latest Collection Stylish Casu...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oricum</td>\n",
       "      <td>Combo pack of 2 casual sneaker shoes for men S...</td>\n",
       "      <td>₹398</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shoes Bank</td>\n",
       "      <td>White Sneaker For Men's/Boy's Sneakers For Men</td>\n",
       "      <td>₹349</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>STYLISH-WHITE-6 Sneakers For Men</td>\n",
       "      <td>₹199</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Oricum</td>\n",
       "      <td>ORIFWSH(OR)-1077 Sneakers For Men</td>\n",
       "      <td>₹214</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>luxury fashion</td>\n",
       "      <td>Luxury sneaker shoes Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>seekort</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹284</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Bonexy</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand                                Product_description  \\\n",
       "0                Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
       "1          Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "2   World Wear Footwear  Combo Pack of 4 Latest Collection Stylish Casu...   \n",
       "3                Oricum  Combo pack of 2 casual sneaker shoes for men S...   \n",
       "4            Shoes Bank     White Sneaker For Men's/Boy's Sneakers For Men   \n",
       "..                  ...                                                ...   \n",
       "95             URBANBOX                   STYLISH-WHITE-6 Sneakers For Men   \n",
       "96               Oricum                  ORIFWSH(OR)-1077 Sneakers For Men   \n",
       "97       luxury fashion              Luxury sneaker shoes Sneakers For Men   \n",
       "98              seekort                                   Sneakers For Men   \n",
       "99               Bonexy                                   Sneakers For Men   \n",
       "\n",
       "   Price Discount  \n",
       "0   ₹474  76% off  \n",
       "1   ₹399  60% off  \n",
       "2   ₹474  76% off  \n",
       "3   ₹398  60% off  \n",
       "4   ₹349  65% off  \n",
       "..   ...      ...  \n",
       "95  ₹199  60% off  \n",
       "96  ₹214  57% off  \n",
       "97  ₹499  83% off  \n",
       "98  ₹284  71% off  \n",
       "99  ₹399  60% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers['Brand']=Brand\n",
    "sneakers['Product_description']=Product_description\n",
    "sneakers['Price']=Price\n",
    "sneakers['Discount']=Discount\n",
    "sneakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Scrape First 100 shoes from myntra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.myntra.com/shoes')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying filter for price\n",
    "btn=driver.find_element_by_xpath(\"//ul[@class='price-list']/li[2]\")\n",
    "btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying filter for color\n",
    "btn1=driver.find_element_by_xpath(\"//li[@class='colour-listItem']/label[1]\")\n",
    "btn1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "Brand=[]\n",
    "Description=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting brand name of the products\n",
    "brand=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "#extracting text from these tags\n",
    "for i in brand:\n",
    "    Brand.append(i.text)\n",
    "    \n",
    "#extracting product description\n",
    "description=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "#extracting text from these tags\n",
    "for i in description:\n",
    "    Description.append(i.text)\n",
    "    \n",
    "#extracting price details of the product\n",
    "price=driver.find_elements_by_xpath(\"//div[@class='product-price']/span[1]\")\n",
    "#exracting text from these tags\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "    \n",
    "#navigating to next page\n",
    "next_page=driver.find_element_by_xpath(\"//li[@class='pagination-next']/a[1]\")\n",
    "next_page.click() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second page\n",
    "#extracting brand name of the products\n",
    "brand=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "#extracting text from these tags\n",
    "for i in brand:\n",
    "    Brand.append(i.text)\n",
    "    \n",
    "#extracting product description\n",
    "description=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "#extracting text from these tags\n",
    "for i in description:\n",
    "    Description.append(i.text)\n",
    "    \n",
    "#extracting price details of the product\n",
    "price=driver.find_elements_by_xpath(\"//div[@class='product-price']/span[1]\")\n",
    "#exracting text from these tags\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product_description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>Rs. 6599Rs. 10999</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>Rs. 8799Rs. 10999</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>Combo Pack of 4 Latest Collection Stylish Casu...</td>\n",
       "      <td>Rs. 9793Rs. 13990</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oricum</td>\n",
       "      <td>Combo pack of 2 casual sneaker shoes for men S...</td>\n",
       "      <td>Rs. 11199Rs. 13999</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shoes Bank</td>\n",
       "      <td>White Sneaker For Men's/Boy's Sneakers For Men</td>\n",
       "      <td>Rs. 9999Rs. 19999</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>STYLISH-WHITE-6 Sneakers For Men</td>\n",
       "      <td>Rs. 6839Rs. 7599</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Oricum</td>\n",
       "      <td>ORIFWSH(OR)-1077 Sneakers For Men</td>\n",
       "      <td>Rs. 8999</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>luxury fashion</td>\n",
       "      <td>Luxury sneaker shoes Sneakers For Men</td>\n",
       "      <td>Rs. 6799Rs. 7999</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>seekort</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>Rs. 9999</td>\n",
       "      <td>71% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Bonexy</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>Rs. 6990</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand                                Product_description  \\\n",
       "0                Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
       "1          Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "2   World Wear Footwear  Combo Pack of 4 Latest Collection Stylish Casu...   \n",
       "3                Oricum  Combo pack of 2 casual sneaker shoes for men S...   \n",
       "4            Shoes Bank     White Sneaker For Men's/Boy's Sneakers For Men   \n",
       "..                  ...                                                ...   \n",
       "95             URBANBOX                   STYLISH-WHITE-6 Sneakers For Men   \n",
       "96               Oricum                  ORIFWSH(OR)-1077 Sneakers For Men   \n",
       "97       luxury fashion              Luxury sneaker shoes Sneakers For Men   \n",
       "98              seekort                                   Sneakers For Men   \n",
       "99               Bonexy                                   Sneakers For Men   \n",
       "\n",
       "                 Price Discount  \n",
       "0    Rs. 6599Rs. 10999  76% off  \n",
       "1    Rs. 8799Rs. 10999  60% off  \n",
       "2    Rs. 9793Rs. 13990  76% off  \n",
       "3   Rs. 11199Rs. 13999  60% off  \n",
       "4    Rs. 9999Rs. 19999  65% off  \n",
       "..                 ...      ...  \n",
       "95    Rs. 6839Rs. 7599  60% off  \n",
       "96            Rs. 8999  57% off  \n",
       "97    Rs. 6799Rs. 7999  83% off  \n",
       "98            Rs. 9999  71% off  \n",
       "99            Rs. 6990  60% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=Brand\n",
    "shoes['Description']=Description\n",
    "sneakers['Price']=Price\n",
    "sneakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10: Scrape first 10 laptops data from amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first connecting to the webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=('https://www.amazon.in/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching the product in the webpage\n",
    "search_product=driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "search_product.send_keys(\"Laptop\")\n",
    "search_btn=driver.find_element_by_xpath(\"//span[@class='nav-search-submit-text nav-sprite nav-progressive-attribute']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying filter for cpu type i7\n",
    "btn=driver.find_element_by_xpath(\"//ul[@class='a-unordered-list a-nostyle a-vertical a-spacing-medium']/li[16]/span[1]\")\n",
    "btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying filter for cpu type i9\n",
    "btn1=driver.find_element_by_xpath(\"//ul[@class='a-unordered-list a-nostyle a-vertical a-spacing-medium']/li[18]/span[1]\")\n",
    "btn1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating empty lists\n",
    "Title=[]\n",
    "Rating=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the title names\n",
    "title=driver.find_elements_by_xpath(\"//h2[@class='a-size-mini a-spacing-none a-color-base s-line-clamp-2']\")\n",
    "#extracting text from these tags\n",
    "for i in title[:10]:\n",
    "    Title.append(i.text)\n",
    "    \n",
    "#extracting the rating\n",
    "soup= BeautifulSoup(driver.page_source, 'html.parser')\n",
    "for d in soup.find_all('div',attrs={'class':'sg-col-4-of-12 sg-col-8-of-16 sg-col-12-of-20 sg-col'}):\n",
    "    rating=d.find('span', attrs={'class':'a-icon-alt'})\n",
    "    if rating is not None:\n",
    "        Rating.append(rating.text)\n",
    "    else:\n",
    "        Rating.append('not_rated')\n",
    "#slicing the rating because each value is obtained twice\n",
    "Rating=Rating[1:20:2]\n",
    "    \n",
    "#extracting price details of the product\n",
    "price=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "#exracting text from these tags\n",
    "for i in price[:10]:\n",
    "    Price.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo Yoga S940 Intel Core i7 10th Gen 14\" UH...</td>\n",
       "      <td>3.3 out of 5 stars</td>\n",
       "      <td>1,19,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i5-1...</td>\n",
       "      <td>4.2 out of 5 stars</td>\n",
       "      <td>50,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...</td>\n",
       "      <td>5.0 out of 5 stars</td>\n",
       "      <td>2,68,325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lenovo Yoga S740 Intel Core i7 10th Gen 14 inc...</td>\n",
       "      <td>3.3 out of 5 stars</td>\n",
       "      <td>1,09,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP Pavilion x360 Touchscreen 2-in-1 FHD 14-inc...</td>\n",
       "      <td>3.5 out of 5 stars</td>\n",
       "      <td>74,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lenovo IdeaPad Gaming 3i 10th Gen Intel Core i...</td>\n",
       "      <td>3.8 out of 5 stars</td>\n",
       "      <td>76,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dell G3 3500 Gaming 15.6inch 120hz FHD Display...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>82,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HP 14 Thin &amp; Light 14-inch FHD Laptop (11th Ge...</td>\n",
       "      <td>4.0 out of 5 stars</td>\n",
       "      <td>75,482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lenovo Legion 5i 10th Gen Intel Core i7 15.6 i...</td>\n",
       "      <td>4.2 out of 5 stars</td>\n",
       "      <td>88,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HP 14 10th Gen Intel Core i7 Ultra Thin and Li...</td>\n",
       "      <td>3.4 out of 5 stars</td>\n",
       "      <td>72,800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title              Rating  \\\n",
       "0  Lenovo Yoga S940 Intel Core i7 10th Gen 14\" UH...  3.3 out of 5 stars   \n",
       "1  Mi Notebook Horizon Edition 14 Intel Core i5-1...  4.2 out of 5 stars   \n",
       "2  ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...  5.0 out of 5 stars   \n",
       "3  Lenovo Yoga S740 Intel Core i7 10th Gen 14 inc...  3.3 out of 5 stars   \n",
       "4  HP Pavilion x360 Touchscreen 2-in-1 FHD 14-inc...  3.5 out of 5 stars   \n",
       "5  Lenovo IdeaPad Gaming 3i 10th Gen Intel Core i...  3.8 out of 5 stars   \n",
       "6  Dell G3 3500 Gaming 15.6inch 120hz FHD Display...  4.0 out of 5 stars   \n",
       "7  HP 14 Thin & Light 14-inch FHD Laptop (11th Ge...  4.0 out of 5 stars   \n",
       "8  Lenovo Legion 5i 10th Gen Intel Core i7 15.6 i...  4.2 out of 5 stars   \n",
       "9  HP 14 10th Gen Intel Core i7 Ultra Thin and Li...  3.4 out of 5 stars   \n",
       "\n",
       "      Price  \n",
       "0  1,19,990  \n",
       "1    50,999  \n",
       "2  2,68,325  \n",
       "3  1,09,990  \n",
       "4    74,990  \n",
       "5    76,990  \n",
       "6    82,490  \n",
       "7    75,482  \n",
       "8    88,990  \n",
       "9    72,800  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now creating a dataframe\n",
    "laptops=pd.DataFrame({})\n",
    "laptops['Title']=Title\n",
    "laptops['Rating']=Rating\n",
    "laptops['Price']=Price\n",
    "laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END OF THE NOTEBOOK FILE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
